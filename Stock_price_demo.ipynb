{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47aebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Stock Price Big Data Analysis v·ªõi PySpark\n",
    "# Ph√¢n t√≠ch d·ªØ li·ªáu ch·ª©ng kho√°n t·ª´ HDFS\n",
    "# ============================================================\n",
    "\n",
    "# Cell 1: Import th∆∞ vi·ªán v√† kh·ªüi t·∫°o Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# T·∫°o Spark Session k·∫øt n·ªëi v·ªõi Spark Master\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Stock Price Big Data Analysis\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Spark Session kh·ªüi t·∫°o th√†nh c√¥ng!\")\n",
    "print(f\"üìå Spark Version: {spark.version}\")\n",
    "print(f\"üìå Spark Master: {spark.sparkContext.master}\")\n",
    "print(f\"üìå Application Name: {spark.sparkContext.appName}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================\n",
    "# Cell 2: ƒê·ªçc d·ªØ li·ªáu t·ª´ HDFS\n",
    "print(\"\\nüìÇ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ HDFS...\")\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a schema cho d·ªØ li·ªáu stock\n",
    "schema = StructType([\n",
    "    StructField(\"Date\", DateType(), True),\n",
    "    StructField(\"Open\", DoubleType(), True),\n",
    "    StructField(\"High\", DoubleType(), True),\n",
    "    StructField(\"Low\", DoubleType(), True),\n",
    "    StructField(\"Close\", DoubleType(), True),\n",
    "    StructField(\"Volume\", LongType(), True)\n",
    "])\n",
    "\n",
    "# ƒê·ªçc t·∫•t c·∫£ file CSV t·ª´ HDFS (path: hdfs://namenode:9000/datack/)\n",
    "# L∆∞u √Ω: ƒê∆∞·ªùng d·∫´n n√†y tr√πng v·ªõi c·∫•u tr√∫c trong README\n",
    "df_stock = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"hdfs://namenode:9000/datack/*.csv\")\n",
    "\n",
    "# Th√™m c·ªôt ticker t·ª´ t√™n file\n",
    "df_stock = df_stock.withColumn(\n",
    "    \"Ticker\",\n",
    "    regexp_extract(input_file_name(), r\"stock_market_data-([A-Z]+)_\", 1)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ ƒë·ªçc d·ªØ li·ªáu t·ª´ HDFS th√†nh c√¥ng!\")\n",
    "print(f\"üìä T·ªïng s·ªë records: {df_stock.count():,}\")\n",
    "print(f\"üìä S·ªë l∆∞·ª£ng c·ªôt: {len(df_stock.columns)}\")\n",
    "print(f\"üìä S·ªë l∆∞·ª£ng c√¥ng ty: {df_stock.select('Ticker').distinct().count()}\")\n",
    "\n",
    "# Hi·ªÉn th·ªã schema\n",
    "print(\"\\nüìã Schema c·ªßa d·ªØ li·ªáu:\")\n",
    "df_stock.printSchema()\n",
    "\n",
    "# Hi·ªÉn th·ªã 10 d√≤ng ƒë·∫ßu ti√™n\n",
    "print(\"\\nüìã 10 d√≤ng d·ªØ li·ªáu ƒë·∫ßu ti√™n:\")\n",
    "df_stock.show(10, truncate=False)\n",
    "\n",
    "# ============================================================\n",
    "# Cell 3: L√†m s·∫°ch v√† x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "print(\"\\nüîß B·∫Øt ƒë·∫ßu l√†m s·∫°ch d·ªØ li·ªáu...\")\n",
    "\n",
    "# Lo·∫°i b·ªè c√°c record c√≥ gi√° tr·ªã null\n",
    "df_clean = df_stock.dropna()\n",
    "\n",
    "# T√≠nh c√°c ch·ªâ s·ªë k·ªπ thu·∫≠t\n",
    "df_clean = df_clean.withColumn(\"Price_Range\", col(\"High\") - col(\"Low\"))\n",
    "df_clean = df_clean.withColumn(\"Daily_Return\", (col(\"Close\") - col(\"Open\")) / col(\"Open\") * 100)\n",
    "df_clean = df_clean.withColumn(\"Year\", year(col(\"Date\")))\n",
    "df_clean = df_clean.withColumn(\"Month\", month(col(\"Date\")))\n",
    "\n",
    "# Cache d·ªØ li·ªáu ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô x·ª≠ l√Ω\n",
    "df_clean.cache()\n",
    "\n",
    "print(f\"‚úÖ L√†m s·∫°ch d·ªØ li·ªáu ho√†n t·∫•t!\")\n",
    "print(f\"üìä S·ªë records sau khi l√†m s·∫°ch: {df_clean.count():,}\")\n",
    "\n",
    "# Hi·ªÉn th·ªã d·ªØ li·ªáu sau khi x·ª≠ l√Ω\n",
    "print(\"\\nüìã D·ªØ li·ªáu sau khi x·ª≠ l√Ω:\")\n",
    "df_clean.select(\"Ticker\", \"Date\", \"Close\", \"Daily_Return\", \"Price_Range\").show(10)\n",
    "\n",
    "# ============================================================\n",
    "# Cell 4: Ph√¢n t√≠ch th·ªëng k√™ c∆° b·∫£n\n",
    "print(\"\\nüìä PH√ÇN T√çCH TH·ªêNG K√ä C∆† B·∫¢N\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Th·ªëng k√™ m√¥ t·∫£\n",
    "print(\"\\n1Ô∏è‚É£ Th·ªëng k√™ m√¥ t·∫£ cho c√°c c·ªôt gi√°:\")\n",
    "df_clean.select(\"Open\", \"High\", \"Low\", \"Close\", \"Volume\").describe().show()\n",
    "\n",
    "# Top 10 c√¥ng ty c√≥ kh·ªëi l∆∞·ª£ng giao d·ªãch cao nh·∫•t\n",
    "print(\"\\n2Ô∏è‚É£ Top 10 c√¥ng ty c√≥ kh·ªëi l∆∞·ª£ng giao d·ªãch trung b√¨nh cao nh·∫•t:\")\n",
    "top_volume = df_clean.groupBy(\"Ticker\") \\\n",
    "    .agg(avg(\"Volume\").alias(\"Avg_Volume\")) \\\n",
    "    .orderBy(desc(\"Avg_Volume\")) \\\n",
    "    .limit(10)\n",
    "top_volume.show()\n",
    "\n",
    "# Top 10 c√¥ng ty c√≥ gi√° ƒë√≥ng c·ª≠a trung b√¨nh cao nh·∫•t\n",
    "print(\"\\n3Ô∏è‚É£ Top 10 c√¥ng ty c√≥ gi√° ƒë√≥ng c·ª≠a trung b√¨nh cao nh·∫•t:\")\n",
    "top_price = df_clean.groupBy(\"Ticker\") \\\n",
    "    .agg(avg(\"Close\").alias(\"Avg_Close_Price\")) \\\n",
    "    .orderBy(desc(\"Avg_Close_Price\")) \\\n",
    "    .limit(10)\n",
    "top_price.show()\n",
    "\n",
    "# Top 10 c√¥ng ty c√≥ bi·∫øn ƒë·ªông gi√° cao nh·∫•t\n",
    "print(\"\\n4Ô∏è‚É£ Top 10 c√¥ng ty c√≥ bi·∫øn ƒë·ªông gi√° cao nh·∫•t:\")\n",
    "top_volatility = df_clean.groupBy(\"Ticker\") \\\n",
    "    .agg(avg(\"Price_Range\").alias(\"Avg_Price_Range\")) \\\n",
    "    .orderBy(desc(\"Avg_Price_Range\")) \\\n",
    "    .limit(10)\n",
    "top_volatility.show()\n",
    "\n",
    "# ============================================================\n",
    "# Cell 5: Ph√¢n t√≠ch xu h∆∞·ªõng theo th·ªùi gian\n",
    "print(\"\\nüìà PH√ÇN T√çCH XU H∆Ø·ªöNG THEO TH·ªúI GIAN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Th·ªëng k√™ theo nƒÉm\n",
    "print(\"\\n1Ô∏è‚É£ Gi√° trung b√¨nh v√† kh·ªëi l∆∞·ª£ng giao d·ªãch theo nƒÉm:\")\n",
    "yearly_stats = df_clean.groupBy(\"Year\") \\\n",
    "    .agg(\n",
    "        avg(\"Close\").alias(\"Avg_Close\"),\n",
    "        avg(\"Volume\").alias(\"Avg_Volume\"),\n",
    "        count(\"*\").alias(\"Total_Records\")\n",
    "    ) \\\n",
    "    .orderBy(\"Year\")\n",
    "yearly_stats.show()\n",
    "\n",
    "# Ph√¢n t√≠ch theo th√°ng\n",
    "print(\"\\n2Ô∏è‚É£ Gi√° trung b√¨nh theo th√°ng (t·∫•t c·∫£ c√°c nƒÉm):\")\n",
    "monthly_stats = df_clean.groupBy(\"Month\") \\\n",
    "    .agg(\n",
    "        avg(\"Close\").alias(\"Avg_Close\"),\n",
    "        avg(\"Daily_Return\").alias(\"Avg_Return\")\n",
    "    ) \\\n",
    "    .orderBy(\"Month\")\n",
    "monthly_stats.show()\n",
    "\n",
    "# ============================================================\n",
    "# Cell 6: Ph√¢n t√≠ch c√°c c·ªï phi·∫øu c·ª• th·ªÉ (AAPL, GOOGL, AMZN, MSFT)\n",
    "print(\"\\nüéØ PH√ÇN T√çCH C√ÅC C·ªî PHI·∫æU QUAN TR·ªåNG\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# L·ªçc c√°c c·ªï phi·∫øu quan tr·ªçng\n",
    "major_stocks = [\"AAPL\", \"GOOGL\", \"AMZN\", \"MSFT\", \"TSLA\"]\n",
    "df_major = df_clean.filter(col(\"Ticker\").isin(major_stocks))\n",
    "\n",
    "print(f\"\\nüìä Ph√¢n t√≠ch {len(major_stocks)} c·ªï phi·∫øu: {', '.join(major_stocks)}\")\n",
    "\n",
    "# Th·ªëng k√™ cho t·ª´ng c·ªï phi·∫øu\n",
    "for ticker in major_stocks:\n",
    "    df_ticker = df_major.filter(col(\"Ticker\") == ticker)\n",
    "    \n",
    "    stats = df_ticker.select(\n",
    "        min(\"Close\").alias(\"Min_Price\"),\n",
    "        max(\"Close\").alias(\"Max_Price\"),\n",
    "        avg(\"Close\").alias(\"Avg_Price\"),\n",
    "        stddev(\"Close\").alias(\"Std_Price\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"\\n{ticker}:\")\n",
    "    print(f\"  - Gi√° th·∫•p nh·∫•t: ${stats['Min_Price']:.2f}\")\n",
    "    print(f\"  - Gi√° cao nh·∫•t: ${stats['Max_Price']:.2f}\")\n",
    "    print(f\"  - Gi√° trung b√¨nh: ${stats['Avg_Price']:.2f}\")\n",
    "    print(f\"  - ƒê·ªô l·ªách chu·∫©n: ${stats['Std_Price']:.2f}\")\n",
    "\n",
    "# ============================================================\n",
    "# Cell 7: Ph√¢n t√≠ch t∆∞∆°ng quan\n",
    "print(\"\\nüîó PH√ÇN T√çCH T∆Ø∆†NG QUAN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# T√≠nh t∆∞∆°ng quan gi·ªØa c√°c bi·∫øn\n",
    "print(\"\\n1Ô∏è‚É£ Ma tr·∫≠n t∆∞∆°ng quan gi·ªØa c√°c bi·∫øn gi√°:\")\n",
    "correlation_cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "\n",
    "# Chuy·ªÉn sang Pandas DataFrame ƒë·ªÉ d·ªÖ t√≠nh correlation\n",
    "df_sample = df_clean.select(correlation_cols).sample(fraction=0.1).toPandas()\n",
    "\n",
    "correlation_matrix = df_sample.corr()\n",
    "print(\"\\n\", correlation_matrix)\n",
    "\n",
    "# ============================================================\n",
    "# Cell 8: Visualization v·ªõi Matplotlib\n",
    "print(\"\\nüìä T·∫†O VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convert m·ªôt ph·∫ßn d·ªØ li·ªáu sang Pandas ƒë·ªÉ visualization\n",
    "df_viz = df_clean.filter(col(\"Ticker\").isin([\"AAPL\", \"GOOGL\", \"MSFT\"])) \\\n",
    "    .filter(col(\"Year\") >= 2018) \\\n",
    "    .select(\"Date\", \"Ticker\", \"Close\") \\\n",
    "    .toPandas()\n",
    "\n",
    "# S·∫Øp x·∫øp theo ng√†y\n",
    "df_viz = df_viz.sort_values(\"Date\")\n",
    "\n",
    "# Plot 1: Line chart so s√°nh gi√° c·ªï phi·∫øu\n",
    "plt.figure(figsize=(14, 6))\n",
    "for ticker in [\"AAPL\", \"GOOGL\", \"MSFT\"]:\n",
    "    data = df_viz[df_viz[\"Ticker\"] == ticker]\n",
    "    plt.plot(data[\"Date\"], data[\"Close\"], label=ticker, linewidth=2)\n",
    "\n",
    "plt.title(\"So s√°nh gi√° c·ªï phi·∫øu AAPL, GOOGL, MSFT (2018-2020)\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Ng√†y\", fontsize=12)\n",
    "plt.ylabel(\"Gi√° ƒë√≥ng c·ª≠a ($)\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/jovyan/work/stock_comparison.png\", dpi=300)\n",
    "print(\"‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: stock_comparison.png\")\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Volume distribution\n",
    "df_volume = top_volume.toPandas()\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(df_volume[\"Ticker\"], df_volume[\"Avg_Volume\"], color='steelblue')\n",
    "plt.xlabel(\"Kh·ªëi l∆∞·ª£ng giao d·ªãch trung b√¨nh\", fontsize=12)\n",
    "plt.title(\"Top 10 c√¥ng ty c√≥ kh·ªëi l∆∞·ª£ng giao d·ªãch cao nh·∫•t\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/jovyan/work/top_volume.png\", dpi=300)\n",
    "print(\"‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: top_volume.png\")\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Heatmap t∆∞∆°ng quan\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title(\"Ma tr·∫≠n t∆∞∆°ng quan gi·ªØa c√°c bi·∫øn\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/jovyan/work/correlation_heatmap.png\", dpi=300)\n",
    "print(\"‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: correlation_heatmap.png\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# Cell 9: L∆∞u k·∫øt qu·∫£ ph√¢n t√≠ch v√†o HDFS\n",
    "print(\"\\nüíæ L∆ØU K·∫æT QU·∫¢ PH√ÇN T√çCH V√ÄO HDFS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# L∆∞u top companies theo volume\n",
    "print(\"\\n1Ô∏è‚É£ L∆∞u top companies by volume...\")\n",
    "top_volume.write.mode(\"overwrite\").parquet(\"hdfs://namenode:9000/output/top_volume\")\n",
    "print(\"‚úÖ ƒê√£ l∆∞u v√†o: hdfs://namenode:9000/output/top_volume\")\n",
    "\n",
    "# L∆∞u yearly statistics\n",
    "print(\"\\n2Ô∏è‚É£ L∆∞u yearly statistics...\")\n",
    "yearly_stats.write.mode(\"overwrite\").parquet(\"hdfs://namenode:9000/output/yearly_stats\")\n",
    "print(\"‚úÖ ƒê√£ l∆∞u v√†o: hdfs://namenode:9000/output/yearly_stats\")\n",
    "\n",
    "# L∆∞u processed data\n",
    "print(\"\\n3Ô∏è‚É£ L∆∞u processed data sample...\")\n",
    "df_clean.filter(col(\"Year\") == 2020).write.mode(\"overwrite\") \\\n",
    "    .parquet(\"hdfs://namenode:9000/output/processed_2020\")\n",
    "print(\"‚úÖ ƒê√£ l∆∞u v√†o: hdfs://namenode:9000/output/processed_2020\")\n",
    "\n",
    "# ============================================================\n",
    "# Cell 10: T·ªïng k·∫øt v√† d·ªçn d·∫πp\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ HO√ÄN TH√ÄNH PH√ÇN T√çCH D·ªÆ LI·ªÜU CH·ª®NG KHO√ÅN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìä T√≥m t·∫Øt k·∫øt qu·∫£:\")\n",
    "print(f\"  ‚Ä¢ T·ªïng s·ªë records ƒë√£ x·ª≠ l√Ω: {df_clean.count():,}\")\n",
    "print(f\"  ‚Ä¢ S·ªë l∆∞·ª£ng c√¥ng ty: {df_clean.select('Ticker').distinct().count()}\")\n",
    "print(f\"  ‚Ä¢ Kho·∫£ng th·ªùi gian: {df_clean.select(min('Date')).collect()[0][0]} - {df_clean.select(max('Date')).collect()[0][0]}\")\n",
    "print(f\"  ‚Ä¢ S·ªë bi·ªÉu ƒë·ªì ƒë√£ t·∫°o: 3\")\n",
    "print(f\"  ‚Ä¢ S·ªë file output tr√™n HDFS: 3\")\n",
    "\n",
    "print(\"\\nüìÅ C√°c file ƒë√£ t·∫°o:\")\n",
    "print(\"  ‚Ä¢ /home/jovyan/work/stock_comparison.png\")\n",
    "print(\"  ‚Ä¢ /home/jovyan/work/top_volume.png\")\n",
    "print(\"  ‚Ä¢ /home/jovyan/work/correlation_heatmap.png\")\n",
    "\n",
    "print(\"\\nüìÅ D·ªØ li·ªáu ƒë√£ l∆∞u tr√™n HDFS:\")\n",
    "print(\"  ‚Ä¢ hdfs://namenode:9000/output/top_volume\")\n",
    "print(\"  ‚Ä¢ hdfs://namenode:9000/output/yearly_stats\")\n",
    "print(\"  ‚Ä¢ hdfs://namenode:9000/output/processed_2020\")\n",
    "\n",
    "# D·ª´ng Spark Session\n",
    "# spark.stop()\n",
    "print(\"\\nüéâ Ho√†n t·∫•t! Spark Session v·∫´n ƒëang ch·∫°y.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
